{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Twitter**"
      ],
      "metadata": {
        "id": "OfJDG__CySiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save to CSV**"
      ],
      "metadata": {
        "id": "ynPF9QLPrh0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def initialize_csv(csv_file='0224.csv'):\n",
        "    # Define the columns we want in our CSV\n",
        "    columns = [\n",
        "        \"id\",\n",
        "        \"conversation_id\",\n",
        "        \"post_type\",\n",
        "        \"created_time\",\n",
        "        \"timestamp\",\n",
        "        \"source\",\n",
        "        \"favorite_count\",\n",
        "        \"retweet_count\",\n",
        "        \"reply_count\",\n",
        "        \"quote_count\",\n",
        "        \"view_count\",\n",
        "        \"text\",\n",
        "        \"text_lang\",\n",
        "        \"text_tagged_users\",\n",
        "        \"author_id\",\n",
        "        \"author_username\",\n",
        "        \"in_reply_to_post_id\",\n",
        "        \"in_reply_to_profile_id\",\n",
        "        \"in_reply_to_profile_username\",\n",
        "        \"quoted_status_id\",\n",
        "        \"quoted_status_profile_id\",\n",
        "        \"quoted_status_profile_username\"\n",
        "    ]\n",
        "\n",
        "    # Create empty DataFrame with these columns\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(csv_file, index=False)\n",
        "    print(f\"Initialized {csv_file} with column headers\")\n",
        "\n",
        "\n",
        "initialize_csv()"
      ],
      "metadata": {
        "id": "-cM_BqrJWr4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77197f34-03cf-4e80-9f64-420b1b0cdb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized 0224.csv with column headers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get Data**"
      ],
      "metadata": {
        "id": "mkOyrJStrb-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def get_all_search_results(keywords, access_token, task_id, max_attempts=12):\n",
        "    results_url = \"https://api.data365.co/v1.1/twitter/search/post/posts\"\n",
        "    all_tweets = []\n",
        "\n",
        "    # Add location parameters to target US cities\n",
        "    params = {\n",
        "        'access_token': access_token,\n",
        "        'task_id': task_id,\n",
        "        'keywords': keywords,\n",
        "        'max_page_size': 500,\n",
        "        'lang': 'en',\n",
        "        'to_date': '2023-09-27T13:38:35'\n",
        "#        'from_date': '2025-02-25T00:00:00'\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            print(f\"Attempt {attempt + 1}/{max_attempts} to get results...\")\n",
        "            response = requests.get(results_url, params=params)\n",
        "\n",
        "            # Debug response\n",
        "            print(f\"Response status code: {response.status_code}\")\n",
        "            print(f\"Response content: {response.text}\")\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error status code: {response.status_code}\")\n",
        "                time.sleep(15)\n",
        "                continue\n",
        "\n",
        "            data = response.json()\n",
        "\n",
        "            if data['status'] == 'ok' and data.get('data', {}).get('items'):\n",
        "                all_tweets.extend(data['data']['items'])\n",
        "                print(f\"Retrieved {len(data['data']['items'])} tweets.\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"No tweets found in response, waiting 10 seconds...\")\n",
        "                time.sleep(15)\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            print(\"Waiting 15 seconds before retry...\")\n",
        "            time.sleep(15)\n",
        "\n",
        "    return all_tweets\n",
        "\n",
        "def append_tweets_to_csv(json_data, csv_file='0224.csv'):\n",
        "    # Convert JSON to list if it's a single dictionary\n",
        "    if isinstance(json_data, dict):\n",
        "        tweets = [json_data]\n",
        "    else:\n",
        "        tweets = json_data\n",
        "\n",
        "    # Create list to store tweet data\n",
        "    tweets_data = []\n",
        "\n",
        "    for tweet in tweets:\n",
        "        tweet_dict = {\n",
        "            \"id\": tweet[\"id\"],\n",
        "            \"conversation_id\": tweet[\"conversation_id\"],\n",
        "            \"post_type\": tweet[\"post_type\"],\n",
        "            \"created_time\": tweet[\"created_time\"],\n",
        "            \"timestamp\": tweet[\"timestamp\"],\n",
        "            \"source\": tweet[\"source\"],\n",
        "            \"favorite_count\": tweet[\"favorite_count\"],\n",
        "            \"retweet_count\": tweet[\"retweet_count\"],\n",
        "            \"reply_count\": tweet[\"reply_count\"],\n",
        "            \"quote_count\": tweet[\"quote_count\"],\n",
        "            \"view_count\": tweet.get(\"view_count\", None),\n",
        "            \"text\": tweet[\"text\"],\n",
        "            \"text_lang\": tweet[\"text_lang\"],\n",
        "            \"text_tagged_users\": tweet.get(\"text_tagged_users\", []),\n",
        "            \"author_id\": tweet[\"author_id\"],\n",
        "            \"author_username\": tweet[\"author_username\"],\n",
        "            \"in_reply_to_post_id\": tweet.get(\"in_reply_to_post_id\"),\n",
        "            \"in_reply_to_profile_id\": tweet.get(\"in_reply_to_profile_id\"),\n",
        "            \"in_reply_to_profile_username\": tweet.get(\"in_reply_to_profile_username\"),\n",
        "            \"quoted_status_id\": tweet.get(\"quoted_status_id\"),\n",
        "            \"quoted_status_profile_id\": tweet.get(\"quoted_status_profile_id\"),\n",
        "            \"quoted_status_profile_username\": tweet.get(\"quoted_status_profile_username\")\n",
        "        }\n",
        "        tweets_data.append(tweet_dict)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(tweets_data)\n",
        "\n",
        "    try:\n",
        "        # Try to read existing CSV file\n",
        "        existing_df = pd.read_csv(csv_file)\n",
        "        # Append new data\n",
        "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
        "        # Remove duplicates based on tweet ID\n",
        "        combined_df = combined_df.drop_duplicates(subset=['id'], keep='last')\n",
        "        # Save back to CSV\n",
        "        combined_df.to_csv(csv_file, index=False)\n",
        "        print(f\"Successfully appended {len(df)} tweets to {csv_file}\")\n",
        "    except FileNotFoundError:\n",
        "        # If file doesn't exist, create new CSV\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"Created new file {csv_file} with {len(df)} tweets\")\n",
        "\n",
        "def save_tweets(tweets, filename):\n",
        "    \"\"\"Save simplified version of tweets\"\"\"\n",
        "    simplified_tweets = []\n",
        "    for tweet in tweets:\n",
        "        simplified_tweets.append({\n",
        "            'id': tweet['id'],\n",
        "            'created_time': tweet['created_time'],\n",
        "            'text': tweet['text'],\n",
        "            'author_username': tweet['author_username'],\n",
        "            'favorite_count': tweet['favorite_count'],\n",
        "            'retweet_count': tweet['retweet_count'],\n",
        "            'reply_count': tweet['reply_count'],\n",
        "            'view_count': tweet['view_count'],\n",
        "            'text_lang': tweet['text_lang']\n",
        "        })\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(simplified_tweets, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return simplified_tweets\n",
        "\n",
        "def main():\n",
        "    access_token = \"ZXlKMGVYQWlPaUpLVjFRaUxDSmhiR2NpT2lKSVV6STFOaUo5LmV5SnpkV0lpT2lKRFRWVWlMQ0pwWVhRaU9qRTNNems0T1RVMk16UXVNalF6T1RNM05YMC5haVR2SDNMYmQxcjgzZ3p0dFhfZWJMSDlyZWJmdVM3NC1fUHd2QWJJamdJ\"\n",
        "    keyword = \"rbc wealth management\"\n",
        "\n",
        "    # Iterate through each keyword\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Processing keyword: {keyword}\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    # STEP 1: Initiate the search\n",
        "    search_url = \"https://api.data365.co/v1.1/twitter/search/post/update\"\n",
        "    search_params = {\n",
        "        'keywords': keyword,\n",
        "        'search_type': 'latest',\n",
        "        'max_posts': 500,\n",
        "        'access_token': access_token,\n",
        "        'lang': 'en',\n",
        "        'to_date': '2023-09-27T13:38:35'\n",
        "#        'from_date': '2025-02-25T00:00:00'\n",
        "    }\n",
        "\n",
        "    print(f\"Initiating search for: {keyword}\")\n",
        "    search_response = requests.post(search_url, params=search_params)\n",
        "\n",
        "    print(f\"Search response: {search_response.text}\")\n",
        "\n",
        "    response_data = search_response.json()\n",
        "    task_id = response_data.get('data', {}).get('task_id')\n",
        "\n",
        "    if not task_id:\n",
        "        print(f\"Error: Could not find task_id in response for keyword: {keyword}\")\n",
        "\n",
        "    print(f\"Search initiated with task_id: {task_id}\")\n",
        "\n",
        "    # STEP 2: Wait before fetching results\n",
        "    print(\"Waiting 200 seconds before fetching results...\")\n",
        "    time.sleep(700)\n",
        "\n",
        "    # STEP 3: Get results\n",
        "    print(f\"Fetching results for task_id: {task_id}\")\n",
        "    tweets = get_all_search_results(keyword, access_token, task_id)\n",
        "\n",
        "    if not tweets:\n",
        "        print(f\"No tweets were retrieved for keyword: {keyword}\")\n",
        "\n",
        "    # Create timestamp for filenames\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save full response with keyword in filename\n",
        "    full_filename = f'full_tweets_{keyword.replace(\" \", \"_\")}_{timestamp}.json'\n",
        "    with open(full_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(tweets, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Append to CSV\n",
        "    with open(full_filename, 'r') as file:\n",
        "        json_data = json.load(file)\n",
        "        append_tweets_to_csv(json_data)\n",
        "\n",
        "    # Save simplified version\n",
        "    simplified_filename = f'simplified_tweets_{keyword.replace(\" \", \"_\")}_{timestamp}.json'\n",
        "    simplified = save_tweets(tweets, simplified_filename)\n",
        "\n",
        "    print(f\"\\nRetrieved {len(tweets)} tweets for keyword: {keyword}\")\n",
        "    print(f\"Saved to '{full_filename}' and '{simplified_filename}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "F3dijg4i-K1v",
        "outputId": "0f9226b7-a523-487d-a81e-a075e26c4667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Processing keyword: rbc wealth management\n",
            "==================================================\n",
            "\n",
            "Initiating search for: rbc wealth management\n",
            "Search response: {\"data\":null,\"error\":{\"code\":\"AuthorizationFailed\",\"message\":\"You have no access to this social network API\",\"target\":\"access_token\",\"value\":null},\"status\":\"fail\"}\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'get'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-95e713a3278b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-95e713a3278b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mtask_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'task_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUk1enQYBmMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6brGRjg_6Klj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}